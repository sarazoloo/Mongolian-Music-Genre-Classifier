{"cells":[{"cell_type":"code","source":"%%writefile [genre_classifier_app].py\n\n#SizeRestrictions_BODY.\nimport streamlit as st\nimport isodate\n\nimport pickle\nimport pytube as pt\nfrom pytube import YouTube\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport random\nfrom python_speech_features import mfcc\nimport scipy.io.wavfile as wav\nfrom tempfile import TemporaryFile\nimport random \nimport operator\nimport math\n\nimport librosa\nimport librosa.display\n\nimport json\n\n\nsns.set(style=\"darkgrid\", color_codes=True)\nst.set_page_config(layout=\"wide\")\n\nst.title('Mongolian Music Genre Classifier')\n\n\nst.header(\"Description\")\nst.markdown(f\"\"\" ### This is a simple music classifier with around 6000 audio files and 6 different genres.\nEach audio file is 30 seconds long. The 6 genres are:\n\\n Pop, mpop, rock, indie, folk, and hiphop.\nAlthough there are a lot of genres mongolian most songs fall into the genres pop, and hiphop.\nIf you are wondering about what mpop is it's mongolian pop, slightly different from \nthe international pop music your used to.\n\"\"\")\nst.markdown(\"##### The model is based on the mfcc put in 10 different segments. Here is sample of how each genre's MFCCs look like\")\nimage_folk = Image.open('/work/folk_MFCC.png')\nimage_hiphop = Image.open('/work/hiphop_MFCC.png')\nimage_pop = Image.open('/work/pop_MFCC.png')\nimage_mpop = Image.open('/work/mpop_MFCC.png')\nimage_rock = Image.open('/work/rock_MFCC.png')\nimage_indie = Image.open('/work/indie_MFCC.png')\nst.image(image_folk, caption =\"Folk genre\", use_column_width=True)\nst.image(image_pop, caption =\"Pop genre\", use_column_width=True)\nst.image(image_mpop, caption =\"Mpop genre\", use_column_width=True)\nst.image(image_rock, caption =\"Rock genre\", use_column_width=True)\nst.image(image_indie, caption =\"Indie genre\", use_column_width=True)\n\n#load model\nann_model = pickle.load(open('ann_model.pkl', 'rb'))\n\n#genre_dict for the output\ngenre_dict = {0:'hiphop', 1:'rock', 2:'mpop', 3:'folk', 4:'pop', 5:'indie'}\n\n#url to mp3 converter\ndef youtube_to_mp3(url):    \n    yt = YouTube(str(url))\n    video = yt.streams.filter(only_audio=True).first()\n    #print(\"Enter the destination address (leave blank to save in current directory)\")\n    #destination = str(input(\" \")) or '.'\n    out_file = video.download(output_path='.')\n    base, ext = os.path.splitext(out_file)\n    new_file = base + '.mp3'\n    os.rename(out_file, new_file)\n    \n    return print(yt.title + \" has been successfully loaded \")\n\nst.header(\"File upload\")\nuploaded_file = st.file_uploader(\"Please upload a .mp3\")\nst.write (f\"\"\" Upload a file or upload a URL\"\"\")\n\ntry:\n    if uploaded_file is not None:\n        audio_file = uploaded_file\n        st.audio( audio_file)\n    else:\n        yt_url = st.text_input(\"Input youtube url\")\n        if len(yt_url) > 0:\n            st.audio(yt_url)\n            audio_file = youtube_to_mp3(yt_url)\nexcept:\n    # print(\"File upload error\")\n    raise\n\n#processing audio file to get mfcc\ndef process_input(audio_file):\n    sample_rate = 22050\n    num_mfcc = 13\n    n_ftt=2048\n    hop_len=512\n    track_dur = 30 # measured in seconds\n    samples_per_track = sample_rate * track_dur\n    num_seg = 10\n    samples_per_segment = int(samples_per_track / num_seg)\n    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_len)\n\n    signal, sample_rate = librosa.load(audio_file, sr=sample_rate)\n    for d in range(10):\n        # calculate start and finish sample for current segment\n        start = samples_per_segment * d\n        finish = start + samples_per_segment\n\n            # extract mfcc\n        mfcc = librosa.feature.mfcc(y=signal[start:finish], sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_ftt, hop_length=hop_len)\n        mfcc = mfcc.T\n\n    return mfcc\n\n    #prediction func\ndef predict(X_predict):\n    X_to_predict = X_predict[np.newaxis, ...]\n\n    predict_prob=ann_model.predict(X_to_predict)\n    predict_classes=np.argmax(predict_prob, axis=1)\n    # get index with max value\n    predicted_index = (predict_classes)\n\n    pred_percen = round((predict_prob.max() * 100), 2)\n\n    #print(\"Predicted Genre:\", genre_dict[int(predicted_index)] , predict_prob.max())\n    pred_genre = genre_dict[int(predicted_index)]\n    pred_prob = predict_prob.max()\n    return pred_genre, pred_percen\n\n        \npredict_button = st.button(\"Predict music genre\", key = '01')\n#prediction button\nif predict_button:\n    input_mfcc = process_input(audio_file)\n    genre, percentage = predict(X_predict=input_mfcc)\n\n    st.write(\"Prediction results\", genre, percentage, \"%\")\n\nelse:\n    st.write('No files uploaded')\n","metadata":{"cell_id":"4b8f4ddbc1df4f22885c60323e009488","source_hash":"83a8631a","execution_start":1684130177592,"execution_millis":43,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Overwriting [genre_classifier_app].py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!streamlit run [genre_classifier_app].py --server.port=8080 --browser.serverAddress='0.0.0.0' --server.enableXsrfProtection=false --server.maxUploadSize=500 --server.enableCORS=false --server.enableWebsocketCompression=false ","metadata":{"cell_id":"a9f52cd8e5434f93a43993e798fdc517","source_hash":"34f09285","execution_start":1684130177635,"execution_millis":307044,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  URL: \u001b[0m\u001b[1mhttp://0.0.0.0:8080\u001b[0m\n\u001b[0m\n2023-05-15 05:57:01.609633: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-15 05:57:01.763416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-05-15 05:57:01.763457: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-05-15 05:57:01.790185: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-05-15 05:57:03.256432: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-05-15 05:57:03.256523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-05-15 05:57:03.256539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-05-15 05:57:05.259481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-15 05:57:05.259519: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-15 05:57:05.259554: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-b07dafd9-52d0-496f-b99d-0bc116229374): /proc/driver/nvidia/version does not exist\n2023-05-15 05:57:05.259772: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n^C\n\u001b[34m  Stopping...\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"cell_id":"a3834f61932d464386620c59e23a1c16","source_hash":"b623e53d","execution_start":1684130484675,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b07dafd9-52d0-496f-b99d-0bc116229374' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"f9392f8c586a4eccb5d9789e85a1b429","deepnote_execution_queue":[]}}